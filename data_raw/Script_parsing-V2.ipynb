{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import re\n",
    "import bleach\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_CHARACTER = \"ROSS\"\n",
    "class Line: \n",
    "    def __init__ (self, speaker, line):\n",
    "        self.speaker = speaker\n",
    "        self.line = line\n",
    "\n",
    "    def __str__ (self):\n",
    "        return self.speaker + \": \" + self.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    step1 = line.strip()\n",
    "    step2 = re.sub(r'\\([^)]*\\)',\"\", step1)\n",
    "    return step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def shorten_pair(pair):\n",
    "    (query, reply) = pair\n",
    "    query_sentences = sent_tokenize(query.line)\n",
    "    reply_sentences = sent_tokenize(reply.line)\n",
    "\n",
    "    new_input = \"\"\n",
    "    num_input = 0\n",
    "    for s in reversed(query_sentences):\n",
    "        s_tokens = word_tokenize(s)\n",
    "        if num_input + len(s_tokens) <= 30:\n",
    "            new_input = s + \" \" + new_input\n",
    "            num_input += len(s_tokens)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    new_input = new_input.strip()\n",
    "\n",
    "    new_reply = \"\"\n",
    "    num_reply = 0\n",
    "    for s in reply_sentences:\n",
    "        s_tokens = word_tokenize(s)\n",
    "        if num_reply + len(s_tokens) <= 30:\n",
    "            new_reply = new_reply + \" \" + s\n",
    "            num_reply += len(s_tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    new_reply = new_reply.strip()\n",
    "    \n",
    "    p1 = Line(query.speaker, new_input)\n",
    "    p2 = Line(reply.speaker, new_reply)\n",
    "    return (p1,p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_pairs(lines):\n",
    "    scene_characters = {}\n",
    "    lines_structured = []\n",
    "    for l in lines:\n",
    "        l_fields = l.split(\":\")\n",
    "        if len(l_fields) == 1:\n",
    "            continue\n",
    "            \n",
    "        character = l_fields[0].strip().upper()\n",
    "        if character not in scene_characters:\n",
    "            scene_characters[character] = 0\n",
    "        scene_characters[character] += 1\n",
    "        character_words = l_fields[1].strip()\n",
    "        lines_structured.append(Line(character, character_words))\n",
    "        \n",
    "    if TARGET_CHARACTER not in scene_characters:\n",
    "        return []\n",
    "    \n",
    "    line_pairs = []\n",
    "    prev_line = lines_structured[0]\n",
    "    for l in lines_structured[1:]:\n",
    "        if l.speaker == TARGET_CHARACTER:\n",
    "            truncated_pair = shorten_pair((prev_line, l))\n",
    "            line_pairs.append(truncated_pair)\n",
    "        elif l.speaker == \"ALL\" and prev_line.speaker != TARGET_CHARACTER:\n",
    "            l_new = Line(TARGET_CHARACTER, l.line)\n",
    "            truncated_pair = shorten_pair((prev_line, l_new))\n",
    "            line_pairs.append(truncated_pair)\n",
    "            \n",
    "        elif TARGET_CHARACTER in l.speaker and prev_line.speaker != TARGET_CHARACTER:\n",
    "            l_new = Line(TARGET_CHARACTER, l.line)\n",
    "            truncated_pair = shorten_pair((prev_line, l_new))\n",
    "            line_pairs.append(truncated_pair)\n",
    "        prev_line = l \n",
    "    \n",
    "    return line_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairs_to_string(pairs):\n",
    "    ret = \"\"\n",
    "    for (p1, p2) in pairs:\n",
    "        ret += str(p1.line) + \" <+++++> \" + str(p2.line) + \"\\n\"\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(file_name):\n",
    "    f = open(file_name, 'r', encoding = \"ISO-8859-1\")\n",
    "    f_contents = f.read()\n",
    "    f.close()\n",
    "    scenes = re.compile(\"\\[.*\\]\").split(f_contents)\n",
    "    \n",
    "    pairs_from_file = \"\"\n",
    "    for scene in scenes:\n",
    "        scene_strip = scene.strip()\n",
    "        if scene_strip == \"\":\n",
    "            continue \n",
    "        scene_lines = scene.split(\"\\n\")\n",
    "        processed_lines = []\n",
    "        for l in scene_lines:\n",
    "            tmp = process_line(l)\n",
    "            if tmp != \"\":\n",
    "                processed_lines.append(tmp)\n",
    "        line_pairs = make_pairs(processed_lines)\n",
    "        pairs_txt = pairs_to_string(line_pairs)\n",
    "        pairs_from_file += pairs_txt\n",
    "    return pairs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scripts = os.listdir(\"scripts/\")\n",
    "all_data = open(\"Ross_responses.txt\", 'w')\n",
    "for s in scripts:\n",
    "    file_data = process_file(\"scripts/\" + s)\n",
    "    if file_data == None:\n",
    "        continue\n",
    "    all_data.write(file_data)\n",
    "all_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition data into 3 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221605\n"
     ]
    }
   ],
   "source": [
    "def divide_data():\n",
    "    f = open(\"Cornell_data_all.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "    data = f.read().split(\"\\n\")[:-1]\n",
    "    print(len(data))\n",
    "    random.Random(1776).shuffle(data)\n",
    "    #8719 pieces of Ross data, (test, valid, train) 1000, 2000, rest\n",
    "    #221605 pieces of Cornell data, (test, valid, train) 44321, 35456, rest\n",
    "    test_data = data[:44321]\n",
    "    valid_data = data[44321:79777]\n",
    "    train_data = data[79777:]\n",
    "    \n",
    "    test_file = open(\"Cornell_test.txt\", 'w')\n",
    "    for d in test_data:\n",
    "        test_file.write(d+\"\\n\")\n",
    "    test_file.close()\n",
    "    \n",
    "    valid_file = open(\"Cornell_valid.txt\", 'w')\n",
    "    for d in valid_data:\n",
    "        valid_file.write(d+\"\\n\")\n",
    "    valid_file.close()\n",
    "    \n",
    "    train_file = open(\"Cornell_train.txt\", 'w')\n",
    "    for d in train_data:\n",
    "        train_file.write(d+\"\\n\")\n",
    "    train_file.close()\n",
    "\n",
    "divide_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35456.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "177284/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79777"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35456+44321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
