{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import re\n",
    "import bleach\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_CHARACTER = \"ROSS\"\n",
    "class Line: \n",
    "    def __init__ (self, speaker, line):\n",
    "        self.speaker = speaker\n",
    "        self.line = line\n",
    "\n",
    "    def __str__ (self):\n",
    "        return self.speaker + \": \" + self.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    step1 = line.strip()\n",
    "    step2 = re.sub(r'\\([^)]*\\)',\"\", step1)\n",
    "    return step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shorten_pair(pair):\n",
    "    (query, reply) = pair\n",
    "    query_sentences = sent_tokenize(query.line)\n",
    "    reply_sentences = sent_tokenize(reply.line)\n",
    "\n",
    "    new_input = \"\"\n",
    "    num_input = 0\n",
    "    for s in reversed(query_sentences):\n",
    "        s_tokens = word_tokenize(s)\n",
    "        if num_input + len(s_tokens) <= 30:\n",
    "            new_input = s + \" \" + new_input\n",
    "            num_input += len(s_tokens)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    new_input = new_input.strip()\n",
    "\n",
    "    new_reply = \"\"\n",
    "    num_reply = 0\n",
    "    for s in reply_sentences:\n",
    "        s_tokens = word_tokenize(s)\n",
    "        if num_reply + len(s_tokens) <= 30:\n",
    "            new_reply = new_reply + \" \" + s\n",
    "            num_reply += len(s_tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    new_reply = new_reply.strip()\n",
    "    \n",
    "    p1 = Line(query.speaker, new_input)\n",
    "    p2 = Line(reply.speaker, new_reply)\n",
    "    return (p1,p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pairs(lines):\n",
    "    scene_characters = {}\n",
    "    lines_structured = []\n",
    "    for l in lines:\n",
    "        l_fields = l.split(\":\")\n",
    "        if len(l_fields) == 1:\n",
    "            continue\n",
    "            \n",
    "        character = l_fields[0].strip().upper()\n",
    "        if character not in scene_characters:\n",
    "            scene_characters[character] = 0\n",
    "        scene_characters[character] += 1\n",
    "        character_words = l_fields[1].strip()\n",
    "        lines_structured.append(Line(character, character_words))\n",
    "        \n",
    "    if TARGET_CHARACTER not in scene_characters:\n",
    "        return []\n",
    "    \n",
    "    line_pairs = []\n",
    "    prev_line = lines_structured[0]\n",
    "    for l in lines_structured[1:]:\n",
    "        if l.speaker == TARGET_CHARACTER:\n",
    "            truncated_pair = shorten_pair((prev_line, l))\n",
    "            line_pairs.append(truncated_pair)\n",
    "        elif l.speaker == \"ALL\" and prev_line.speaker != TARGET_CHARACTER:\n",
    "            l_new = Line(TARGET_CHARACTER, l.line)\n",
    "            truncated_pair = shorten_pair((prev_line, l_new))\n",
    "            line_pairs.append(truncated_pair)\n",
    "            \n",
    "        elif TARGET_CHARACTER in l.speaker and prev_line.speaker != TARGET_CHARACTER:\n",
    "            l_new = Line(TARGET_CHARACTER, l.line)\n",
    "            truncated_pair = shorten_pair((prev_line, l_new))\n",
    "            line_pairs.append(truncated_pair)\n",
    "        prev_line = l \n",
    "    \n",
    "    return line_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairs_to_string(pairs):\n",
    "    ret = \"\"\n",
    "    for (p1, p2) in pairs:\n",
    "        ret += str(p1.line) + \" <+++++> \" + str(p2.line) + \"\\n\"\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(file_name):\n",
    "    f = open(file_name, 'r', encoding = \"ISO-8859-1\")\n",
    "    f_contents = f.read()\n",
    "    f.close()\n",
    "    scenes = re.compile(\"\\[.*\\]\").split(f_contents)\n",
    "    \n",
    "    pairs_from_file = \"\"\n",
    "    for scene in scenes:\n",
    "        scene_strip = scene.strip()\n",
    "        if scene_strip == \"\":\n",
    "            continue\n",
    "        scene_lines = scene.split(\"\\n\")\n",
    "        processed_lines = []\n",
    "        for l in scene_lines:\n",
    "            tmp = process_line(l)\n",
    "            if tmp != \"\":\n",
    "                processed_lines.append(tmp)\n",
    "        line_pairs = make_pairs(processed_lines)\n",
    "        pairs_txt = pairs_to_string(line_pairs)\n",
    "        pairs_from_file += pairs_txt\n",
    "    return pairs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scripts = os.listdir(\"scripts/\")\n",
    "all_data = open(\"Ross_all.txt\", 'w')\n",
    "for s in scripts:\n",
    "    file_data = process_file(\"scripts/\" + s)\n",
    "    if file_data == None:\n",
    "        continue\n",
    "    all_data.write(file_data)\n",
    "all_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_line(text):\n",
    "    text = re.sub('\\x85','...', text)\n",
    "    text = re.sub('\\x91','\\'', text)\n",
    "    text = re.sub('\\x92','\\'', text)\n",
    "    text = re.sub('\\x96', '-', text)\n",
    "    text = re.sub('\\x97', '-', text)\n",
    "    text = re.sub('Ã‚', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(target_file_name, data):\n",
    "    base = \"../data/\"\n",
    "    f_query = open(base+target_file_name+\"_query.en\", 'w')\n",
    "    f_reply = open(base+target_file_name+\"_reply.en\", 'w')\n",
    "    for d in data:\n",
    "        d_clean = clean_line(d)\n",
    "        pair = d_clean.split(\" <+++++> \")\n",
    "        if pair[0].strip() == \"\" or pair[1].strip() == \"\":\n",
    "            continue \n",
    "        f_query.write(\"\\n\"+pair[0])\n",
    "        f_reply.write(\"\\n\"+pair[1])\n",
    "    f_query.close()\n",
    "    f_reply.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Ross():\n",
    "    f = open(\"Ross_all.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    data = f.read().split(\"\\n\")[:-1]\n",
    "    random.Random(1776).shuffle(data)\n",
    "    \n",
    "    test = data[:1500]\n",
    "    valid = data[1500:2500]\n",
    "    train = data[2500:]\n",
    "    \n",
    "    write_to_file(\"Ross_test\", test)\n",
    "    write_to_file(\"Ross_valid\", valid)\n",
    "    write_to_file(\"Ross_train\", train)\n",
    "\n",
    "split_Ross()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def split_Cornell():\n",
    "    f = open(\"Cornell_all.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    data = f.read().split(\"\\n\")[:-1]\n",
    "    random.Random(1776).shuffle(data)\n",
    "    \n",
    "    test = data[:1500]\n",
    "    valid_2 = data[1500:2500]\n",
    "    train_2 = data[2500:8719]\n",
    "    \n",
    "    valid_1 = data[8719:45000]\n",
    "    train_1 = data[45000:]\n",
    "    \n",
    "    write_to_file(\"Cornell_test\", test)\n",
    "    write_to_file(\"Cornell_valid_2\", valid_2)\n",
    "    write_to_file(\"Cornell_train_2\", train_2)\n",
    "    \n",
    "    write_to_file(\"Cornell_valid\", valid_1)\n",
    "    write_to_file(\"Cornell_train\", train_1)\n",
    "    \n",
    "split_Cornell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
