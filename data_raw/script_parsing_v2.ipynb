{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import re\n",
    "import bleach\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_CHARACTER = \"ROSS\"\n",
    "class Line: \n",
    "    def __init__ (self, speaker, line):\n",
    "        self.speaker = speaker\n",
    "        self.line = line\n",
    "\n",
    "    def __str__ (self):\n",
    "        return self.speaker + \": \" + self.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    step1 = line.strip()\n",
    "    step2 = re.sub(r'\\([^)]*\\)',\"\", step1)\n",
    "    return step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shorten_pair(pair):\n",
    "    (query, reply) = pair\n",
    "    query_sentences = sent_tokenize(query.line)\n",
    "    reply_sentences = sent_tokenize(reply.line)\n",
    "\n",
    "    new_input = \"\"\n",
    "    num_input = 0\n",
    "    for s in reversed(query_sentences):\n",
    "        s_tokens = word_tokenize(s)\n",
    "        if num_input + len(s_tokens) <= 30:\n",
    "            new_input = s + \" \" + new_input\n",
    "            num_input += len(s_tokens)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    new_input = new_input.strip()\n",
    "\n",
    "    new_reply = \"\"\n",
    "    num_reply = 0\n",
    "    for s in reply_sentences:\n",
    "        s_tokens = word_tokenize(s)\n",
    "        if num_reply + len(s_tokens) <= 30:\n",
    "            new_reply = new_reply + \" \" + s\n",
    "            num_reply += len(s_tokens)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    new_reply = new_reply.strip()\n",
    "    \n",
    "    p1 = Line(query.speaker, new_input)\n",
    "    p2 = Line(reply.speaker, new_reply)\n",
    "    return (p1,p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pairs(lines):\n",
    "    scene_characters = {}\n",
    "    lines_structured = []\n",
    "    for l in lines:\n",
    "        l_fields = l.split(\":\")\n",
    "        if len(l_fields) == 1:\n",
    "            continue\n",
    "            \n",
    "        character = l_fields[0].strip().upper()\n",
    "        if character not in scene_characters:\n",
    "            scene_characters[character] = 0\n",
    "        scene_characters[character] += 1\n",
    "        character_words = l_fields[1].strip()\n",
    "        lines_structured.append(Line(character, character_words))\n",
    "        \n",
    "    if TARGET_CHARACTER not in scene_characters:\n",
    "        return []\n",
    "    \n",
    "    line_pairs = []\n",
    "    prev_line = lines_structured[0]\n",
    "    for l in lines_structured[1:]:\n",
    "        if l.speaker == TARGET_CHARACTER:\n",
    "            truncated_pair = shorten_pair((prev_line, l))\n",
    "            line_pairs.append(truncated_pair)\n",
    "        elif l.speaker == \"ALL\" and prev_line.speaker != TARGET_CHARACTER:\n",
    "            l_new = Line(TARGET_CHARACTER, l.line)\n",
    "            truncated_pair = shorten_pair((prev_line, l_new))\n",
    "            line_pairs.append(truncated_pair)\n",
    "            \n",
    "        elif TARGET_CHARACTER in l.speaker and prev_line.speaker != TARGET_CHARACTER:\n",
    "            l_new = Line(TARGET_CHARACTER, l.line)\n",
    "            truncated_pair = shorten_pair((prev_line, l_new))\n",
    "            line_pairs.append(truncated_pair)\n",
    "        prev_line = l \n",
    "    \n",
    "    return line_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairs_to_string(pairs):\n",
    "    ret = \"\"\n",
    "    for (p1, p2) in pairs:\n",
    "        ret += str(p1.line) + \" <+++++> \" + str(p2.line) + \"\\n\"\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(file_name):\n",
    "    f = open(file_name, 'r', encoding = \"ISO-8859-1\")\n",
    "    f_contents = f.read()\n",
    "    f.close()\n",
    "    scenes = re.compile(\"\\[.*\\]\").split(f_contents)\n",
    "    \n",
    "    pairs_from_file = \"\"\n",
    "    for scene in scenes:\n",
    "        scene_strip = scene.strip()\n",
    "        if scene_strip == \"\":\n",
    "            continue\n",
    "        scene_lines = scene.split(\"\\n\")\n",
    "        processed_lines = []\n",
    "        for l in scene_lines:\n",
    "            tmp = process_line(l)\n",
    "            if tmp != \"\":\n",
    "                processed_lines.append(tmp)\n",
    "        line_pairs = make_pairs(processed_lines)\n",
    "        pairs_txt = pairs_to_string(line_pairs)\n",
    "        pairs_from_file += pairs_txt\n",
    "    return pairs_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scripts = os.listdir(\"scripts/\")\n",
    "all_data = open(\"Ross_all.txt\", 'w')\n",
    "for s in scripts:\n",
    "    file_data = process_file(\"scripts/\" + s)\n",
    "    if file_data == None:\n",
    "        continue\n",
    "    all_data.write(file_data)\n",
    "all_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_line(text):\n",
    "    text = re.sub('\\x85','...', text)\n",
    "    text = re.sub('\\x91','\\'', text)\n",
    "    text = re.sub('\\x92','\\'', text)\n",
    "    text = re.sub('\\x96', '-', text)\n",
    "    text = re.sub('\\x97', '-', text)\n",
    "    text = re.sub('Ã‚', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(q_file, r_file, data):\n",
    "    for d in data:\n",
    "        d_clean = clean_line(d)\n",
    "        pair = d_clean.split(\" <+++++> \")\n",
    "        q_file.write(pair[0]+\"\\n\")\n",
    "        r_file.write(pair[1]+\"\\n\")\n",
    "\n",
    "def split_in_out(file_name):\n",
    "    f = open(file_name+\"_all.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    data = f.read().split(\"\\n\")[:-1]\n",
    "    random.Random(1776).shuffle(data)\n",
    "    \n",
    "    s1 = data[:45000]\n",
    "    s2 = data[45000:80000]\n",
    "    s3 = data[80000:]\n",
    "    \n",
    "    end_point = \"../data/\" + file_name\n",
    "    s1_query = open(end_point+\"_test_query.en\",'w')\n",
    "    s1_reply = open(end_point+\"_test_reply.en\",'w')\n",
    "    write_to_file(s1_query, s1_reply, s1)\n",
    "    s1_query.close()\n",
    "    s1_reply.close()\n",
    "    \n",
    "    s2_query = open(end_point+\"_valid_query.en\",'w')\n",
    "    s2_reply = open(end_point+\"_valid_reply.en\",'w')\n",
    "    write_to_file(s2_query, s2_reply, s2)\n",
    "    s2_query.close()\n",
    "    s2_reply.close()\n",
    "    \n",
    "    s3_query = open(end_point+\"_train_query.en\",'w')\n",
    "    s3_reply = open(end_point+\"_train_reply.en\",'w')\n",
    "    write_to_file(s3_query, s3_reply, s3)\n",
    "    s3_query.close()\n",
    "    s3_reply.close()\n",
    "    \n",
    "        \n",
    "split_in_out(\"Cornell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221605\n"
     ]
    }
   ],
   "source": [
    "def divide_data():\n",
    "    cat = \"Cornell\"\n",
    "    f = open(cat+\"_all.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "    data = f.read().split(\"\\n\")[:-1]\n",
    "    print(len(data))\n",
    "    random.Random(1776).shuffle(data)\n",
    "    #8719 pieces of Ross data, (test, valid, train) 1000, 1000, 6719\n",
    "    #221605 pieces of Cornell data, (test, valid, train) 44321, 35456(79777), rest\n",
    "    test_data = data[6719:44321]\n",
    "    valid_data = data[44321:79777]\n",
    "    train_1_data = data[79777:]\n",
    "    train_2_data = data[:6719]\n",
    "\n",
    "    \n",
    "    test_file = open(\"../data/\" + cat + \"_test.txt\", 'w')\n",
    "    for d in test_data:\n",
    "        d = clean_line(d)\n",
    "        test_file.write(d+\"\\n\")\n",
    "    test_file.close()\n",
    "    \n",
    "    valid_file = open(\"../data/\"+ cat+\"_valid.txt\", 'w')\n",
    "    for d in valid_data:\n",
    "        d = clean_line(d)\n",
    "        valid_file.write(d+\"\\n\")\n",
    "    valid_file.close()\n",
    "    \n",
    "    train_file = open(\"../data/\"+ cat+ \"_1_train.txt\", 'w')\n",
    "    for d in train_1_data:\n",
    "        d = clean_line(d)\n",
    "        train_file.write(d+\"\\n\")\n",
    "    train_file.close()\n",
    "    \n",
    "    train_file = open(\"../data/\"+ cat+ \"_2_train.txt\", 'w')\n",
    "    for d in train_2_data:\n",
    "        d = clean_line(d)\n",
    "        train_file.write(d+\"\\n\")\n",
    "    train_file.close()\n",
    "\n",
    "divide_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/multimodal_keras_wrapper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Alex/RossBot/data_raw'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multimodal_keras_wrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b2ce61facdf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmultimodal_keras_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'multimodal_keras_wrapper'"
     ]
    }
   ],
   "source": [
    "from multimodal_keras_wrapper.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
